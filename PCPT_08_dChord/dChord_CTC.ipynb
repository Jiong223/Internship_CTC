{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d44f8960-8c38-4c82-936f-98d0a21f12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import libfmp.b\n",
    "import libfmp.c3\n",
    "import libfmp.c4\n",
    "import libfmp.c5\n",
    "%matplotlib inline\n",
    "\n",
    "data_basedir = os.path.join('.', 'data', 'dChord')\n",
    "\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52467d29-0db7-4f95-b7f2-d63d42d9e744",
   "metadata": {},
   "source": [
    "## Network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b01ba9-c92a-4dd6-91c7-6c28c20de456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code copied from libdl.nn_models.cnns_chord_recog\n",
    "\n",
    "class log_compression(torch.nn.Module):\n",
    "    \"\"\"Module for logarithmic compression of an array\n",
    "\n",
    "    Args:\n",
    "        gamma: Compression factor\n",
    "        trainable: Whether the gradient w.r.t. gamma is computed in backward pass\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma_init=1, trainable=True):\n",
    "        super(log_compression, self).__init__()\n",
    "\n",
    "        # define logarithm of gamma as trainable parameter\n",
    "        if gamma_init is not None:\n",
    "            self.log_gamma = torch.nn.parameter.Parameter(data=torch.log(torch.as_tensor(gamma_init, dtype=torch.float32)), requires_grad=trainable)\n",
    "        else:\n",
    "            self.log_gamma = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.log_gamma is not None:\n",
    "            return torch.log(1.0 + torch.exp(self.log_gamma) * x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class gaussian_filter(torch.nn.Module):\n",
    "    \"\"\"Module for generating a 1D Gaussian filter\n",
    "\n",
    "    Args:\n",
    "        length: kernel length\n",
    "        sigma: (initial) standard deviation for Gaussian kernel\n",
    "        dim: dimension across which to apply the filter; output tensor will have singleton dimension at 'dim'\n",
    "        trainable: whether to optimize the standard deviation\n",
    "    \"\"\"\n",
    "    def __init__(self, length=41, sigma_init=1, dim=2, trainable=True):\n",
    "        super(gaussian_filter, self).__init__()\n",
    "\n",
    "        self.length = length\n",
    "        self.dim = dim\n",
    "\n",
    "        if sigma_init is None:\n",
    "            sigma_init = length\n",
    "        self.log_sigma = torch.nn.parameter.Parameter(data=torch.log(torch.as_tensor(sigma_init, dtype=torch.float32)), requires_grad=trainable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        idx = x.ndim * [1]\n",
    "        idx[self.dim] = self.length\n",
    "        w = self.get_kernel()\n",
    "        x_smoothed = torch.sum(x * w.view(*idx), dim=self.dim, keepdim=True)\n",
    "        return x_smoothed\n",
    "\n",
    "    def get_kernel(self):\n",
    "        n = torch.arange(0, self.length).to(self.log_sigma.device) - (self.length - 1.0) / 2.0\n",
    "        sig2 = 2 * torch.exp(self.log_sigma) ** 2\n",
    "        w = torch.exp(-n ** 2 / sig2)\n",
    "        return w / torch.sum(w)\n",
    "\n",
    "\n",
    "class temporal_smoothing(torch.nn.Module):\n",
    "    \"\"\"Module for temporal smoothing of a feature sequence.\n",
    "\n",
    "    Args:\n",
    "        smoothing_type: Either 'weighted_sum', 'median' or 'Gaussian'\n",
    "        avg_length: Length to be averaged over; only relevant for 'weighted_sum' and 'Gaussian' (median is taken over whole input length)\n",
    "        weight_init: How to initialize (trainable) weights, only relevant for 'weighted_sum' (either 'uniform' or 'random')\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing_type='weighted_sum', avg_length=41, weight_init='uniform', sigma_init=20):\n",
    "        super(temporal_smoothing, self).__init__()\n",
    "        if smoothing_type not in {'weighted_sum', 'median', 'Gaussian'}:\n",
    "            raise ValueError('Smoothing type ' + smoothing_type + ' is unknown!')\n",
    "\n",
    "        if weight_init not in {'random', 'uniform'}:\n",
    "            raise ValueError('Weight initialization ' + weight_init + ' is unknown!')\n",
    "\n",
    "        self.avg_length = avg_length\n",
    "\n",
    "        if smoothing_type == 'weighted_sum':\n",
    "            self.filter = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(avg_length, 1),\n",
    "                                    stride=(1, 1), bias=False, padding='valid')\n",
    "\n",
    "            if weight_init == 'random':\n",
    "                pass\n",
    "            elif weight_init == 'uniform':\n",
    "                self.filter.weight.data = torch.ones_like(self.filter.weight.data) / avg_length\n",
    "\n",
    "        elif smoothing_type == 'median':\n",
    "            self.filter = lambda x: torch.median(x, dim=2, keepdim=True,).values\n",
    "\n",
    "        elif smoothing_type == 'Gaussian':\n",
    "            self.filter = gaussian_filter(length=avg_length, sigma_init=sigma_init, dim=2, trainable=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # treat channels as batch dimension to use conv. layer with 1 channel\n",
    "        x_reshaped = x.view(-1, 1, *x.shape[2:])\n",
    "        x_filtered = self.filter(x_reshaped)\n",
    "        return x_filtered.view(*x.shape[:2], *x_filtered.shape[2:])\n",
    "\n",
    "\n",
    "class feature_normalization(torch.nn.Module):\n",
    "    \"\"\"Module for feature normalization\n",
    "\n",
    "    Args:\n",
    "        num_features: Number of features\n",
    "        norm: The norm to be applied. '1', '2', 'max' or 'z'\n",
    "        threshold: Threshold below which the vector `v` is used instead of normalization\n",
    "        v: Used instead of normalization below `threshold`. If None, uses unit vector for given norm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features=12, norm='2', threshold=1e-4, v=None, dim=3):\n",
    "        super(feature_normalization, self).__init__()\n",
    "        if norm not in ['1', '2', 'max', 'z']:\n",
    "            raise ValueError('Norm ' + norm + ' is unknown!')\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.v = v\n",
    "\n",
    "        if norm == '1':\n",
    "            if self.v is None:\n",
    "                self.v = torch.ones(num_features, dtype=torch.float32) / num_features\n",
    "            self.get_norms = lambda x: torch.linalg.vector_norm(x, ord=1.0, dim=dim, keepdim=False)\n",
    "            self.normalize = lambda x: F.normalize(x, p=1.0, dim=dim, eps=threshold)\n",
    "\n",
    "        if norm == '2':\n",
    "            if self.v is None:\n",
    "                self.v = torch.ones(num_features, dtype=torch.float32) / torch.sqrt(torch.tensor([num_features]))\n",
    "            self.get_norms = lambda x: torch.linalg.vector_norm(x, ord=2.0, dim=dim, keepdim=False)\n",
    "            self.normalize = lambda x: F.normalize(x, p=2.0, dim=dim, eps=threshold)\n",
    "\n",
    "        if norm == 'max':\n",
    "            if self.v is None:\n",
    "                self.v = torch.ones(num_features, dtype=torch.float32)\n",
    "            self.get_norms = lambda x: torch.linalg.vector_norm(x, ord=float('inf'), dim=dim, keepdim=False)\n",
    "            self.normalize = lambda x: F.normalize(x, p=float('inf'), dim=dim, eps=threshold)\n",
    "\n",
    "        if norm == 'z':\n",
    "            if self.v is None:\n",
    "                self.v = torch.zeros(num_features)\n",
    "            self.get_norms = lambda x: torch.std(x, dim=dim, keepdim=False, unbiased=True)\n",
    "            self.normalize = lambda x: (x - torch.mean(x, dim=dim, keepdim=True)) / torch.std(x, dim=dim, keepdim=True,\n",
    "                                                                                              unbiased=True)\n",
    "    def forward(self, x):\n",
    "        x_norms = self.get_norms(x)\n",
    "        idx = x_norms > self.threshold\n",
    "        x_normalized = x.clone()\n",
    "        x_normalized = self.normalize(x_normalized)\n",
    "        x_normalized[~idx] = self.v.to(x.device)\n",
    "        return x_normalized\n",
    "\n",
    "\n",
    "class chord_recognition_templates(torch.nn.Module):\n",
    "    \"\"\"Module for applying template-based chord recognition to chroma features.\n",
    "\n",
    "    Args:\n",
    "        shared_weights: Whether to use 2 shared kernels (maj/min) or 24 individual chord templates\n",
    "        initialize_parameters: Whether to initialize kernels with idealized binary chord templates and zero bias\n",
    "        normalize_weights: Whether to normalize all templates to unit Euclidean norm\n",
    "        bias: Whether to allow for a trainable bias\n",
    "    \"\"\"\n",
    "    def __init__(self, shared_weights=True, initialize_parameters=True, normalize_weights=True, bias=False):\n",
    "        super(chord_recognition_templates, self).__init__()\n",
    "\n",
    "        if shared_weights:\n",
    "            self.padding = lambda x: F.pad(x, pad=(0, 11, 0, 0), mode='circular')\n",
    "            self.filter = torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(1, 12), stride=(1, 1), bias=bias)\n",
    "            if initialize_parameters:\n",
    "                self.filter.weight.data = torch.tensor([[[[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]],     # major\n",
    "                                                        [[[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]]]],    # minor\n",
    "                                                       dtype=torch.float32)\n",
    "                if self.filter.bias is not None:\n",
    "                    self.filter.bias.data = torch.zeros_like(self.filter.bias.data, dtype=torch.float32)\n",
    "\n",
    "        else:\n",
    "            self.padding = lambda x: x\n",
    "            self.filter = torch.nn.Conv2d(in_channels=1, out_channels=24, kernel_size=(1, 12), stride=(1, 1), bias=bias)\n",
    "            if initialize_parameters:\n",
    "                kernel_major = torch.tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
    "                kernel_minor = torch.tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n",
    "\n",
    "                for i in range(12):\n",
    "                    self.filter.weight.data[i, 0, 0, :] = torch.roll(kernel_major, i)\n",
    "                    self.filter.weight.data[12 + i, 0, 0, :] = torch.roll(kernel_minor, i)\n",
    "\n",
    "                if self.filter.bias is not None:\n",
    "                    self.filter.bias.data = torch.zeros_like(self.filter.bias.data, dtype=torch.float32)\n",
    "\n",
    "        if normalize_weights:\n",
    "            self.filter.weight.data = F.normalize(self.filter.weight.data, p=2.0, dim=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_padded = self.padding(x)                                          # out: (B x 1 x T x 23) or (B x 1 x T x 12)\n",
    "        y = self.filter(x_padded)                                           # out: (B x 2 x T x 12) or (B x 24 x T x 1)\n",
    "        y_reshaped = torch.swapaxes(y, 1, 2)                                # out: (B x T x 2 x 12) or (B x T x 24 x 1)\n",
    "        y_reshaped = torch.unsqueeze(torch.flatten(y_reshaped, start_dim=2), 1)             # out: (B x 1 x T x 24)\n",
    "        return y_reshaped\n",
    "\n",
    "\n",
    "class softmax_temperature(torch.nn.Module):\n",
    "    \"\"\"Softmax activation with trainable temperature parameter\n",
    "\n",
    "    Args:\n",
    "        dim: Dimension across which to apply the softmax function\n",
    "        tau: Temperature parameter\n",
    "        trainable: Whether the gradient w.r.t. tau is computed in backward pass\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=3, tau=1, trainable=True):\n",
    "        super(softmax_temperature, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.tau = torch.nn.parameter.Parameter(data=torch.tensor([tau], dtype=torch.float32), requires_grad=trainable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(x / self.tau, dim=self.dim)\n",
    "\n",
    "\n",
    "class chord_recog_templates_pipeline(torch.nn.Module):\n",
    "    \"\"\"Model for template-based chord recognition:\n",
    "\n",
    "    Pipeline:\n",
    "        1) input: chroma features\n",
    "        2) log compression (gamma trainable)\n",
    "        3) temporal averaging (conv. layer with trainable weights / median filter / Gaussian filter)\n",
    "        4) l1/l2/max/z normalization (only if norm large enough; otherwise: unit-norm vector with same values)\n",
    "        5) chord recognition via templates (conv. layer)\n",
    "        6) softmax -> output: chord probabilities\n",
    "\n",
    "    Args:\n",
    "        dictionaries containing parameters for the individual building blocks\n",
    "    \"\"\"\n",
    "    def __init__(self, compression_params=None, temp_smooth_params=None, feature_norm_params=None,\n",
    "                 chord_template_params=None, softmax_params=None):\n",
    "\n",
    "        super(chord_recog_templates_pipeline, self).__init__()\n",
    "\n",
    "        self.log_compression = log_compression(**compression_params)\n",
    "        self.temporal_smoothing = temporal_smoothing(**temp_smooth_params)\n",
    "        self.feature_normalization = feature_normalization(**feature_norm_params)\n",
    "        self.chord_recognition_templates = chord_recognition_templates(**chord_template_params)\n",
    "        self.softmax_temperature = softmax_temperature(**softmax_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred, _, _, _, _ = self.get_intermediate_data(x)\n",
    "        return y_pred\n",
    "\n",
    "    def get_intermediate_data(self, x):\n",
    "        x_comp = self.log_compression(x)                                    # out: (B x 1 x T x 12)\n",
    "        x_avg = self.temporal_smoothing(x_comp)                             # out: (B x 1 x 1 x 12)\n",
    "        x_normalized = self.feature_normalization(x_avg)                    # out: (B x 1 x 1 x 12)\n",
    "        x_templates = self.chord_recognition_templates(x_normalized)        # out: (B x 1 x 1 x 24)\n",
    "        y_pred = self.softmax_temperature(x_templates)                      # out: (B x 1 x 1 x 24)\n",
    "        return y_pred, x_templates, x_normalized, x_avg, x_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57b5954-1b89-4d47-be67-0f1366f53c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline config\n",
    "\n",
    "compression_params = {'gamma_init': 1.0,       # makes pipeline somehow redundant; input chroma features are already logarithmically compressed\n",
    "                      'trainable': True}\n",
    "\n",
    "temp_smooth_params = {'smoothing_type': 'weighted_sum', \n",
    "                      'avg_length': 51, \n",
    "                      'weight_init': 'uniform'}\n",
    "\n",
    "# temp_smooth_params = {'smoothing_type': 'Gaussian', \n",
    "#                       'avg_length': 51, \n",
    "#                       'sigma_init': 50}\n",
    "\n",
    "feature_norm_params = {'num_features': 12,\n",
    "                       'norm': '2', \n",
    "                       'threshold': 1e-4}\n",
    "\n",
    "chord_template_params = {'shared_weights': True, \n",
    "                         'initialize_parameters': True, \n",
    "                         'normalize_weights': False}\n",
    "\n",
    "softmax_params = {'tau': 1, \n",
    "                  'trainable': False}\n",
    "\n",
    "model_pipeline = chord_recog_templates_pipeline(compression_params=compression_params, \n",
    "                                       temp_smooth_params=temp_smooth_params, \n",
    "                                       feature_norm_params=feature_norm_params, \n",
    "                                       chord_template_params=chord_template_params, \n",
    "                                       softmax_params=softmax_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7730da12-3d4b-433e-aee0-eab3f73ad9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "chord_recog_templates_pipeline           [1, 1, 1, 24]             --\n",
       "├─log_compression: 1-1                   [1, 1, 51, 12]            1\n",
       "├─temporal_smoothing: 1-2                [1, 1, 1, 12]             --\n",
       "│    └─Conv2d: 2-1                       [1, 1, 1, 12]             51\n",
       "├─feature_normalization: 1-3             [1, 1, 1, 12]             --\n",
       "├─chord_recognition_templates: 1-4       [1, 1, 1, 24]             --\n",
       "│    └─Conv2d: 2-2                       [1, 2, 1, 12]             24\n",
       "├─softmax_temperature: 1-5               [1, 1, 1, 24]             (1)\n",
       "==========================================================================================\n",
       "Total params: 77\n",
       "Trainable params: 76\n",
       "Non-trainable params: 1\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_pipeline, input_size=(1, 1, temp_smooth_params['avg_length'], 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603878f2-1b66-4876-9e21-5e0eed15d010",
   "metadata": {},
   "source": [
    "## The END of network configuration\n",
    "## -------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621ef69-18cc-400b-8228-a66c5f4b85c3",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bae9734c-e276-4757-9f3e-7544aa4ea40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\FAU\\2526SS\\Research_Internship\\PCPT\\PCPT_bundle\\PCPT_08_dChord\\data\\dChord\\FMP_C5_F01_Beatles_LetItBe-mm1-4_Original_Chords_simplified.csv\n"
     ]
    }
   ],
   "source": [
    "nonchord = False\n",
    "song_dict = {}\n",
    "# song_dict[0] = ['LetItB', 'r',\n",
    "#                 os.path.join(data_basedir, 'C5', 'FMP_C5_Audio_Beatles_LetItBe_Beatles_1970-LetItBe-06.wav'),\n",
    "#                 os.path.join(data_basedir, 'C5', 'FMP_C5_Audio_Beatles_LetItBe_Beatles_1970-LetItBe-06_Chords_simplified.csv')]\n",
    "song_dict[0] = ['LetItB', 'r',\n",
    "                os.path.join(data_basedir, 'FMP_C5_F01_Beatles_LetItBe-mm1-4_Original.wav'),\n",
    "                os.path.join(data_basedir, 'FMP_C5_F01_Beatles_LetItBe-mm1-4_Original_Chords_simplified.csv')]\n",
    "song_dict[1] = ['HereCo', 'b',\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_HereComesTheSun_Beatles_1969-AbbeyRoad-07.wav'),\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_HereComesTheSun_Beatles_1969-AbbeyRoad-07_Chords_simplified.csv')]\n",
    "song_dict[2] = ['ObLaDi', 'c',\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_ObLaDiObLaDa_Beatles_1968-TheBeatlesTheWhiteAlbumDisc1-04.wav'),\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_ObLaDiObLaDa_Beatles_1968-TheBeatlesTheWhiteAlbumDisc1-04_Chords_simplified.csv')]\n",
    "song_dict[3] = ['PennyL', 'g',\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_PennyLane_Beatles_1967-MagicalMysteryTour-09.wav'),\n",
    "                os.path.join(data_basedir, 'FMP_C5_Audio_Beatles_PennyLane_Beatles_1967-MagicalMysteryTour-09_Chords_simplified.csv')]\n",
    "\n",
    "# csv_rel_path = song_dict[0][3]\n",
    "# csv_abs_path = os.path.abspath(csv_rel_path)\n",
    "# print(csv_abs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab115dc0-7282-4db3-a249-28e9dd0104dd",
   "metadata": {},
   "source": [
    "### Feature Extraction (log-compressed STFT-based chroma features as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc893d2e-5108-46d9-a5b8-fc2e53aff900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Computation of STFT-based chromagrams =====\n",
      "Processing:  LetItB\n",
      "Processing:  HereCo\n",
      "Processing:  ObLaDi\n",
      "Processing:  PennyL\n"
     ]
    }
   ],
   "source": [
    "def compute_X_dict(song_selected, version='STFT', details=True, nonchord=False):\n",
    "    X_dict = {}\n",
    "    Fs_X_dict = {}\n",
    "    ann_dict = {}\n",
    "    x_dur_dict = {}\n",
    "    chord_labels = libfmp.c5.get_chord_labels(ext_minor='m', nonchord=nonchord)\n",
    "    for s in song_selected:\n",
    "        if details is True:\n",
    "            print('Processing: ', song_dict[s][0])\n",
    "        fn_wav = song_dict[s][2]\n",
    "        fn_ann = song_dict[s][3]\n",
    "#         N = 2048\n",
    "#         H = 1024\n",
    "        N = 4096\n",
    "        H = 2048\n",
    "        if version == 'STFT':\n",
    "            X, Fs_X, x, Fs, x_dur = \\\n",
    "                libfmp.c5.compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=0.1, version='STFT')            # no log compression\n",
    "        if version == 'CQT':\n",
    "            X, Fs_X, x, Fs, x_dur = \\\n",
    "                libfmp.c5.compute_chromagram_from_filename(fn_wav, H=H, version='CQT')                  # no log compression\n",
    "        if version == 'IIR':\n",
    "            X, Fs_X, x, Fs, x_dur = \\\n",
    "                libfmp.c5.compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=10, version='IIR')             # no log compression\n",
    "            \n",
    "        X_dict[s] = X\n",
    "        Fs_X_dict[s] = Fs_X\n",
    "        x_dur_dict[s] = x_dur\n",
    "        N_X = X.shape[1]\n",
    "        \n",
    "        # one-hot encoding, converts chord label (.csv file) to label matrix\n",
    "        ann_dict[s] = libfmp.c5.convert_chord_ann_matrix(fn_ann, chord_labels, Fs=Fs_X, N=N_X, last=False)   \n",
    "        \n",
    "    return X_dict, Fs_X_dict, ann_dict, x_dur_dict, chord_labels\n",
    "    \n",
    "song_selected = [0, 1, 2, 3]\n",
    "#song_selected = [0]\n",
    "print('===== Computation of STFT-based chromagrams =====')\n",
    "X_dict_STFT, Fs_X_dict_STFT, ann_dict_STFT, x_dur_dict, chord_labels = compute_X_dict(song_selected, version='STFT', nonchord=nonchord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31357ce-3d70-492e-8b42-c0f14b00d415",
   "metadata": {},
   "source": [
    "### Create Dataset + Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30350ded-fc39-409a-bbaa-dbb0282d259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_context(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets, params):\n",
    "        self.inputs = inputs                                # Channels x Time x Chromas\n",
    "        self.targets = targets                              # Time x Chords\n",
    "        self.context = params['context']\n",
    "        self.stride = params['stride']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.inputs.size()[1] - self.context) // self.stride\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index *= self.stride\n",
    "        half_context = self.context // 2\n",
    "        index += half_context\n",
    "        X = self.inputs[:, (index - half_context):(index + half_context + 1), :].type(torch.FloatTensor)\n",
    "        # y is the training target(this is strongly aligned label)\n",
    "        y = torch.unsqueeze(torch.unsqueeze(self.targets[index, :], 0), 1).type(torch.FloatTensor)\n",
    "        return X, y\n",
    "    \n",
    "def create_dataset(data_dict, ann_dict, song_dict, song_indices, dataset_params, dataset_description='train'):\n",
    "    all_datasets = []\n",
    "    half_context = dataset_params['context']//2\n",
    "    \n",
    "    for s in song_indices:\n",
    "        inputs = torch.unsqueeze(torch.from_numpy(np.pad(data_dict[s].T, ((half_context, half_context + 1), (0, 0)))), 0)\n",
    "        print(ann_dict[s][0])\n",
    "        # transpose and zero-padding the label matrix\n",
    "        targets = torch.from_numpy(np.pad(ann_dict[s][0].T, ((half_context, half_context + 1), (0, 0))))\n",
    "        curr_dataset = dataset_context(inputs, targets, dataset_params)\n",
    "        all_datasets.append(curr_dataset)\n",
    "        \n",
    "        print(f'- {song_dict[s][0]} added to {dataset_description} set. Length: {len(curr_dataset)} segments')\n",
    "        \n",
    "    full_dataset = torch.utils.data.ConcatDataset(all_datasets)     \n",
    "    print(f'Total number of segments in the {dataset_description} set: {len(full_dataset)}')    \n",
    "    return full_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2ac7dd4-fcc4-49c3-bc37-7ee4336ed548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "- HereCo added to train set. Length: 1999 segments\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "- ObLaDi added to train set. Length: 2034 segments\n",
      "Total number of segments in the train set: 4033\n",
      "\n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "- PennyL added to val set. Length: 1975 segments\n",
      "Total number of segments in the val set: 1975\n",
      "\n",
      "\n",
      "-Training data loader contains 81 mini batches.\n",
      "-Validation data loader contains 40 mini batches.\n"
     ]
    }
   ],
   "source": [
    "train_set_indices = [1, 2]\n",
    "train_set_params = {'context': temp_smooth_params['avg_length'], 'stride': 1}\n",
    "\n",
    "train_set = create_dataset(data_dict=X_dict_STFT, \n",
    "                           ann_dict=ann_dict_STFT, \n",
    "                           song_dict=song_dict, \n",
    "                           song_indices=train_set_indices, \n",
    "                           dataset_params=train_set_params, \n",
    "                           dataset_description='train')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "val_set_indices = [3]\n",
    "val_set_params = {'context': temp_smooth_params['avg_length'], 'stride': 1}\n",
    "\n",
    "val_set = create_dataset(data_dict=X_dict_STFT, \n",
    "                         ann_dict=ann_dict_STFT, \n",
    "                         song_dict=song_dict, \n",
    "                         song_indices=val_set_indices, \n",
    "                         dataset_params=val_set_params, \n",
    "                         dataset_description='val')\n",
    "print('\\n')\n",
    "train_loader_params = {'batch_size': 50, 'shuffle': True, 'num_workers': 0}\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **train_loader_params)\n",
    "print(f'-Training data loader contains {len(train_loader)} mini batches.')\n",
    "\n",
    "val_loader_params = {'batch_size': 50, 'shuffle': False, 'num_workers': 0}\n",
    "val_loader = torch.utils.data.DataLoader(val_set, **val_loader_params)\n",
    "print(f'-Validation data loader contains {len(val_loader)} mini batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a387db1-3d2e-47f7-99a7-6f5ae4f1207d",
   "metadata": {},
   "source": [
    "## Train Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "751b204c-9ceb-4774-9eca-669d22e37676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "\n",
      "'log_compression.log_gamma'\n",
      "'temporal_smoothing.filter.weight'\n",
      "'chord_recognition_templates.filter.weight'\n"
     ]
    }
   ],
   "source": [
    "print('Trainable parameters:\\n')\n",
    "\n",
    "trainable_parameters = []\n",
    "for name, param in model_pipeline.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_parameters.append(name)\n",
    "        print(f\"'{name}'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bbf2cc-0f76-4f6b-b428-ea74f40fe55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_mode = 0\n",
    "\n",
    "if optimization_mode == 1:\n",
    "    frozen_parameters = []           \n",
    "\n",
    "if optimization_mode == 2:\n",
    "    trained_parameters = []\n",
    "if optimization_mode == 0:\n",
    "    parameters_to_optimize = model_pipeline.parameters()\n",
    "    \n",
    "elif optimization_mode == 1:\n",
    "    parameters_to_optimize = []\n",
    "\n",
    "    for name, param in model_pipeline.named_parameters():\n",
    "        if not name in frozen_parameters:\n",
    "            parameters_to_optimize.append(param)\n",
    "            \n",
    "elif optimization_mode == 2:\n",
    "    parameters_to_optimize = []\n",
    "\n",
    "    for name, param in model_pipeline.named_parameters():\n",
    "        if name in trained_parameters:\n",
    "            parameters_to_optimize.append(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44144859-fc7b-44a3-9795-d6e6fc2f929f",
   "metadata": {},
   "source": [
    "### Optimizer&Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b8d740-f325-4ade-8694-77d6b72bf850",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_params = {'lr': 0.01, 'betas': (0.9, 0.999)}\n",
    "optimizer = torch.optim.Adam(parameters_to_optimize, **optim_params)\n",
    "training_params = {'device': 'cpu', 'max_epochs': 50}\n",
    "# training_params = {'device': 'cuda:0', 'max_epochs': 25}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1ee4c-fe74-47f7-8b8c-922140d7b216",
   "metadata": {},
   "source": [
    "# CTC LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a61da83f-ed64-4da3-9bbe-1107fb5762b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ctc_loss_fn(log_probs, targets, input_lengths, target_lengths):\n",
    "    return F.ctc_loss(\n",
    "        log_probs=log_probs,\n",
    "        targets=targets,\n",
    "        input_lengths=input_lengths,\n",
    "        target_lengths=target_lengths,\n",
    "        blank=0,                 # 0 as blank index\n",
    "        reduction='mean',\n",
    "        zero_infinity=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b81cfb-214e-4a1b-a0f6-e19a9eb4b5fe",
   "metadata": {},
   "source": [
    "### Network Training - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e604a039-5146-40c8-8c47-15ca96e28995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0. Train Loss: 2.5137, Val Loss: 2.6907\n",
      "    ...saved model\n",
      "Finished epoch 1. Train Loss: 2.5111, Val Loss: 2.6875\n",
      "    ...saved model\n",
      "Finished epoch 2. Train Loss: 2.5089, Val Loss: 2.6891\n",
      "Finished epoch 3. Train Loss: 2.5061, Val Loss: 2.6880\n",
      "Finished epoch 4. Train Loss: 2.5030, Val Loss: 2.6882\n",
      "Finished epoch 5. Train Loss: 2.5012, Val Loss: 2.6888\n",
      "Finished epoch 6. Train Loss: 2.4987, Val Loss: 2.6820\n",
      "    ...saved model\n",
      "Finished epoch 7. Train Loss: 2.4967, Val Loss: 2.6824\n",
      "Finished epoch 8. Train Loss: 2.4950, Val Loss: 2.6871\n",
      "Finished epoch 9. Train Loss: 2.4930, Val Loss: 2.6820\n",
      "Finished epoch 10. Train Loss: 2.4914, Val Loss: 2.6823\n",
      "Finished epoch 11. Train Loss: 2.4890, Val Loss: 2.6813\n",
      "    ...saved model\n",
      "Finished epoch 12. Train Loss: 2.4875, Val Loss: 2.6789\n",
      "    ...saved model\n",
      "Finished epoch 13. Train Loss: 2.4858, Val Loss: 2.6786\n",
      "    ...saved model\n",
      "Finished epoch 14. Train Loss: 2.4850, Val Loss: 2.6772\n",
      "    ...saved model\n",
      "Finished epoch 15. Train Loss: 2.4830, Val Loss: 2.6785\n",
      "Finished epoch 16. Train Loss: 2.4810, Val Loss: 2.6782\n",
      "Finished epoch 17. Train Loss: 2.4793, Val Loss: 2.6795\n",
      "Finished epoch 18. Train Loss: 2.4785, Val Loss: 2.6770\n",
      "    ...saved model\n",
      "Finished epoch 19. Train Loss: 2.4763, Val Loss: 2.6740\n",
      "    ...saved model\n",
      "Finished epoch 20. Train Loss: 2.4756, Val Loss: 2.6782\n",
      "Finished epoch 21. Train Loss: 2.4742, Val Loss: 2.6769\n",
      "Finished epoch 22. Train Loss: 2.4733, Val Loss: 2.6781\n",
      "Finished epoch 23. Train Loss: 2.4712, Val Loss: 2.6779\n",
      "Finished epoch 24. Train Loss: 2.4708, Val Loss: 2.6757\n",
      "Finished epoch 25. Train Loss: 2.4695, Val Loss: 2.6769\n",
      "Finished epoch 26. Train Loss: 2.4683, Val Loss: 2.6758\n",
      "Finished epoch 27. Train Loss: 2.4676, Val Loss: 2.6764\n",
      "Finished epoch 28. Train Loss: 2.4663, Val Loss: 2.6761\n",
      "Finished epoch 29. Train Loss: 2.4656, Val Loss: 2.6750\n",
      "Finished epoch 30. Train Loss: 2.4642, Val Loss: 2.6746\n",
      "Finished epoch 31. Train Loss: 2.4633, Val Loss: 2.6752\n",
      "Finished epoch 32. Train Loss: 2.4624, Val Loss: 2.6738\n",
      "    ...saved model\n",
      "Finished epoch 33. Train Loss: 2.4618, Val Loss: 2.6705\n",
      "    ...saved model\n",
      "Finished epoch 34. Train Loss: 2.4614, Val Loss: 2.6716\n",
      "Finished epoch 35. Train Loss: 2.4604, Val Loss: 2.6746\n",
      "Finished epoch 36. Train Loss: 2.4593, Val Loss: 2.6736\n",
      "Finished epoch 37. Train Loss: 2.4586, Val Loss: 2.6752\n",
      "Finished epoch 38. Train Loss: 2.4571, Val Loss: 2.6693\n",
      "    ...saved model\n",
      "Finished epoch 39. Train Loss: 2.4565, Val Loss: 2.6729\n",
      "Finished epoch 40. Train Loss: 2.4558, Val Loss: 2.6717\n",
      "Finished epoch 41. Train Loss: 2.4557, Val Loss: 2.6705\n",
      "Finished epoch 42. Train Loss: 2.4546, Val Loss: 2.6692\n",
      "    ...saved model\n",
      "Finished epoch 43. Train Loss: 2.4542, Val Loss: 2.6725\n",
      "Finished epoch 44. Train Loss: 2.4528, Val Loss: 2.6710\n",
      "Finished epoch 45. Train Loss: 2.4524, Val Loss: 2.6701\n",
      "Finished epoch 46. Train Loss: 2.4519, Val Loss: 2.6681\n",
      "    ...saved model\n",
      "Finished epoch 47. Train Loss: 2.4513, Val Loss: 2.6704\n",
      "Finished epoch 48. Train Loss: 2.4511, Val Loss: 2.6715\n",
      "Finished epoch 49. Train Loss: 2.4501, Val Loss: 2.6691\n",
      "\n",
      "Restored model from epoch 46.\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = model_pipeline.to(training_params['device'])\n",
    "\n",
    "best_model = deepcopy(model_pipeline)\n",
    "best_val_loss = None\n",
    "best_epoch = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(training_params['max_epochs']):\n",
    "    model_pipeline.train()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        accum_loss, n_batches = 0, 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(training_params['device']), y.to(training_params['device'])\n",
    "\n",
    "            logits = model_pipeline(X)                       # [B, 1, T, 24]\n",
    "            logits = logits.squeeze(1)                       # [B, T, 24]\n",
    "            log_probs = logits.log_softmax(dim=2)            # [B, T, 24]\n",
    "            log_probs = log_probs.permute(1, 0, 2)           # [T, B, 24]\n",
    "\n",
    "            # Convert one-hot targets to class indices\n",
    "            target_classes = torch.argmax(y.squeeze(1).squeeze(1), dim=1)  # [B]\n",
    "            targets = target_classes\n",
    "\n",
    "            input_lengths = torch.full(\n",
    "                size=(log_probs.size(1),), fill_value=log_probs.size(0), dtype=torch.long\n",
    "            )\n",
    "            target_lengths = torch.ones(log_probs.size(1), dtype=torch.long)\n",
    "\n",
    "            loss = ctc_loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accum_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    model_pipeline.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accum_loss_val, n_batches_val = 0, 0\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(training_params['device']), y_val.to(training_params['device'])\n",
    "\n",
    "            logits_val = model_pipeline(X_val)                       # [B, 1, T, 24]\n",
    "            logits_val = logits_val.squeeze(1)                       # [B, T, 24]\n",
    "            log_probs_val = logits_val.log_softmax(dim=2)            # [B, T, 24]\n",
    "            log_probs_val = log_probs_val.permute(1, 0, 2)           # [T, B, 24]\n",
    "\n",
    "            target_classes_val = torch.argmax(y_val.squeeze(1).squeeze(1), dim=1)  # [B]\n",
    "            targets_val = target_classes_val\n",
    "\n",
    "            input_lengths_val = torch.full(\n",
    "                size=(log_probs_val.size(1),), fill_value=log_probs_val.size(0), dtype=torch.long\n",
    "            )\n",
    "            target_lengths_val = torch.ones(log_probs_val.size(1), dtype=torch.long)\n",
    "\n",
    "            loss_val = ctc_loss_fn(log_probs_val, targets_val, input_lengths_val, target_lengths_val)\n",
    "\n",
    "            accum_loss_val += loss_val.item()\n",
    "            n_batches_val += 1\n",
    "\n",
    "    train_loss = accum_loss / n_batches\n",
    "    val_loss = accum_loss_val / n_batches_val\n",
    "    print(f'Finished epoch {epoch}. Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if best_val_loss is None or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = deepcopy(model_pipeline)\n",
    "        best_epoch = epoch\n",
    "        print('    ...saved model')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "model_pipeline = deepcopy(best_model)\n",
    "print(f'\\nRestored model from epoch {best_epoch}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68c33ed7-153f-41f9-9d80-5deb39ebd710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(train_losses) > 1:\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
